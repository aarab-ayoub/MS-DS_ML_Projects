{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb9dd66",
   "metadata": {},
   "source": [
    "# TP1: Image Classification using Plant Leaf Dataset\n",
    "\n",
    "This notebook implements an image classification solution using the 300_dataset, which contains images of different plant species organized in folders. Each folder represents a class (plant species).\n",
    "\n",
    "## Objectives\n",
    "- Load and preprocess the image dataset\n",
    "- Extract features from the images\n",
    "- Train a classification model\n",
    "- Evaluate the model's performance\n",
    "- Document the approach and results\n",
    "\n",
    "## Dataset Structure\n",
    "The dataset is organized in folders, with each folder representing a different plant species:\n",
    "- Apta\n",
    "- Indian Rubber Tree\n",
    "- Karanj\n",
    "- Kashid\n",
    "- Nilgiri\n",
    "- Pimpal\n",
    "- Sita Ashok\n",
    "- Sonmohar\n",
    "- Vad\n",
    "- Vilayati Chinch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c0110",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0502d0",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "First, let's explore the dataset to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "dataset_path = \"300_dataset\"\n",
    "\n",
    "# List all class folders\n",
    "class_folders = os.listdir(dataset_path)\n",
    "print(f\"Total number of classes: {len(class_folders)}\")\n",
    "print(f\"Class names: {class_folders}\")\n",
    "\n",
    "# Count images per class\n",
    "class_counts = {}\n",
    "for class_name in class_folders:\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        class_counts[class_name] = len(image_files)\n",
    "\n",
    "# Display class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Images per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate total number of images\n",
    "total_images = sum(class_counts.values())\n",
    "print(f\"Total number of images: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0a6cf",
   "metadata": {},
   "source": [
    "### Visualize Sample Images from Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef96cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample image from each class\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, class_name in enumerate(class_counts.keys()):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if image_files:\n",
    "            # Get the first image file\n",
    "            sample_image_path = os.path.join(class_path, image_files[0])\n",
    "            # Read the image\n",
    "            img = cv2.imread(sample_image_path)\n",
    "            # Convert from BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Display the image\n",
    "            plt.subplot(4, 3, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(class_name)\n",
    "            plt.axis('off')\n",
    "            \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e8493",
   "metadata": {},
   "source": [
    "### Check Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584cf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image dimensions to determine a standard size for preprocessing\n",
    "image_dimensions = []\n",
    "for class_name in class_counts.keys():\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if image_files:\n",
    "            # Get a random sample of images (up to 5)\n",
    "            sample_files = np.random.choice(image_files, min(5, len(image_files)), replace=False)\n",
    "            for img_file in sample_files:\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    image_dimensions.append(img.shape)\n",
    "\n",
    "# Display some of the image dimensions\n",
    "print(\"Sample of image dimensions (height, width, channels):\")\n",
    "for i, dim in enumerate(image_dimensions[:10]):\n",
    "    print(f\"Image {i+1}: {dim}\")\n",
    "\n",
    "# Calculate average dimensions\n",
    "heights = [dim[0] for dim in image_dimensions]\n",
    "widths = [dim[1] for dim in image_dimensions]\n",
    "avg_height = int(np.mean(heights))\n",
    "avg_width = int(np.mean(widths))\n",
    "\n",
    "print(f\"\\nAverage height: {avg_height} pixels\")\n",
    "print(f\"Average width: {avg_width} pixels\")\n",
    "\n",
    "# Define target image size for preprocessing\n",
    "# We'll use a standard size to ensure uniform input to our model\n",
    "target_size = (224, 224)  # Standard size for many CNN architectures\n",
    "print(f\"\\nTarget image size for our model: {target_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3881a",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Loading\n",
    "\n",
    "Now we'll prepare the data for our model. We'll use Keras' ImageDataGenerator for data augmentation and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define image preprocessing parameters\n",
    "target_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Define transformations for training (with augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(target_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define transformations for validation (no augmentation)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(target_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom dataset class for loading images from folders\n",
    "class PlantLeafDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Collect all image paths and their labels\n",
    "        for cls_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[cls_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Open image with PIL\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"300_dataset\"\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = PlantLeafDataset(dataset_path, transform=None)\n",
    "\n",
    "# Count images per class\n",
    "class_counts = {}\n",
    "for cls_name in full_dataset.classes:\n",
    "    class_dir = os.path.join(dataset_path, cls_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        class_counts[cls_name] = len(image_files)\n",
    "\n",
    "# Display class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Images per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate total number of images\n",
    "total_images = sum(class_counts.values())\n",
    "print(f\"Total number of images: {total_images}\")\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply transforms to the split datasets\n",
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_dataset = TransformedSubset(train_dataset, train_transforms)\n",
    "val_dataset = TransformedSubset(val_dataset, val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Print class information\n",
    "class_indices = full_dataset.class_to_idx\n",
    "class_names = {v: k for k, v in class_indices.items()}\n",
    "print(\"Class indices:\")\n",
    "for class_name, idx in class_indices.items():\n",
    "    print(f\"{class_name}: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed444f36",
   "metadata": {},
   "source": [
    "### Visualize Augmented Images\n",
    "\n",
    "Let's see how our data augmentation affects the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc800343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and labels from the training loader\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Convert from tensor to numpy for display\n",
    "images_np = images.numpy()\n",
    "\n",
    "# Function to unnormalize the images\n",
    "def imshow(img):\n",
    "    img = img.transpose((1, 2, 0))\n",
    "    # Unnormalize\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "# Plot a few augmented images\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(min(9, images.shape[0])):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(imshow(images_np[i]))\n",
    "    plt.title(f'Class: {class_names[labels[i].item()]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddafb7",
   "metadata": {},
   "source": [
    "## 3. Model Building\n",
    "\n",
    "We'll build two models:\n",
    "1. A simple CNN from scratch\n",
    "2. A transfer learning model using VGG16 pretrained on ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fe541",
   "metadata": {},
   "source": [
    "### 3.1 Simple CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple CNN model in PyTorch\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 512),  # Adjusted based on input size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the simple CNN model\n",
    "num_classes = len(class_indices)\n",
    "simple_cnn_model = SimpleCNN(num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(simple_cnn_model.parameters())\n",
    "\n",
    "# Display model summary\n",
    "print(simple_cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610b46e",
   "metadata": {},
   "source": [
    "### 3.2 Transfer Learning with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecce5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# Assuming device is already defined (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Create a transfer learning model using VGG16 in PyTorch\n",
    "def create_transfer_learning_model(num_classes):\n",
    "    # Load pre-trained VGG16 model\n",
    "    vgg16 = models.vgg16(pretrained=True)\n",
    "    \n",
    "    # Freeze the base model parameters\n",
    "    for param in vgg16.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace the classifier\n",
    "    num_features = vgg16.classifier[6].in_features\n",
    "    vgg16.classifier[6] = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    return vgg16\n",
    "\n",
    "# Initialize the transfer learning model\n",
    "transfer_model = create_transfer_learning_model(num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# Only optimize the classifier parameters that are not frozen\n",
    "transfer_criterion = nn.CrossEntropyLoss()\n",
    "transfer_optimizer = optim.Adam(filter(lambda p: p.requires_grad, transfer_model.parameters()))\n",
    "\n",
    "# Display model summary\n",
    "print(transfer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f1cc3",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "We'll train both models and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255b4bb",
   "metadata": {},
   "source": [
    "### 4.1 Train the Simple CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc283a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for PyTorch model\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25, patience=5):\n",
    "    # Initialize history dictionary to store metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct / total\n",
    "        \n",
    "        # Store metrics\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the simple CNN model\n",
    "simple_cnn_model, history_simple = train_model(\n",
    "    simple_cnn_model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    num_epochs=25,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fe6a5",
   "metadata": {},
   "source": [
    "### 4.2 Train the Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c672740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the transfer learning model\n",
    "transfer_model, history_transfer = train_model(\n",
    "    transfer_model, \n",
    "    transfer_criterion, \n",
    "    transfer_optimizer, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    num_epochs=15,  # Transfer learning usually requires fewer epochs\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef88cbf",
   "metadata": {},
   "source": [
    "### 4.3 Plot Training History for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for both models\n",
    "def plot_training_history(history1, history2, title1, title2):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history1['train_acc'], label=f'{title1} - Training')\n",
    "    plt.plot(history1['val_acc'], label=f'{title1} - Validation')\n",
    "    plt.plot(history2['train_acc'], label=f'{title2} - Training')\n",
    "    plt.plot(history2['val_acc'], label=f'{title2} - Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history1['train_loss'], label=f'{title1} - Training')\n",
    "    plt.plot(history1['val_loss'], label=f'{title1} - Validation')\n",
    "    plt.plot(history2['train_loss'], label=f'{title2} - Training')\n",
    "    plt.plot(history2['val_loss'], label=f'{title2} - Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history_simple, history_transfer, 'Simple CNN', 'Transfer Learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77dbc6",
   "metadata": {},
   "source": [
    "### 4.4 Evaluate Both Models on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe35104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    eval_loss = running_loss / len(data_loader.dataset)\n",
    "    eval_acc = correct / total\n",
    "    \n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "# Evaluate the simple CNN model\n",
    "simple_cnn_evaluation = evaluate_model(simple_cnn_model, val_loader)\n",
    "print(f\"Simple CNN - Validation Loss: {simple_cnn_evaluation[0]:.4f}, Validation Accuracy: {simple_cnn_evaluation[1]:.4f}\")\n",
    "\n",
    "# Evaluate the transfer learning model\n",
    "transfer_evaluation = evaluate_model(transfer_model, val_loader)\n",
    "print(f\"Transfer Learning - Validation Loss: {transfer_evaluation[0]:.4f}, Validation Accuracy: {transfer_evaluation[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909913d",
   "metadata": {},
   "source": [
    "### 4.5 Generate Predictions and Confusion Matrix\n",
    "\n",
    "Let's use the better performing model to generate predictions and visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the better performing model (usually the transfer learning model)\n",
    "better_model = transfer_model if transfer_evaluation[1] > simple_cnn_evaluation[1] else simple_cnn_model\n",
    "model_name = \"Transfer Learning\" if transfer_evaluation[1] > simple_cnn_evaluation[1] else \"Simple CNN\"\n",
    "\n",
    "# Get predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "better_model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = better_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_pred = np.array(all_preds)\n",
    "y_true = np.array(all_labels)\n",
    "\n",
    "# Generate classification report\n",
    "print(f\"Classification Report for {model_name} Model:\")\n",
    "class_names_list = [class_names[i] for i in range(len(class_names))]\n",
    "print(classification_report(y_true, y_pred, target_names=class_names_list))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names_list, \n",
    "            yticklabels=class_names_list)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - {model_name} Model')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4184e79",
   "metadata": {},
   "source": [
    "## 5. Visualize Predictions\n",
    "\n",
    "Let's visualize some predictions from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "# Get a batch of validation images and labels\n",
    "dataiter = iter(val_loader)\n",
    "X_val, y_val = next(dataiter)\n",
    "\n",
    "# Move to device\n",
    "X_val = X_val.to(device)\n",
    "\n",
    "# Make predictions\n",
    "better_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = better_model(X_val)\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "\n",
    "# Convert tensors to numpy for visualization\n",
    "X_val_np = X_val.cpu().numpy()\n",
    "y_val_np = y_val.cpu().numpy()\n",
    "predictions_np = predictions.cpu().numpy()\n",
    "\n",
    "# Plot images with true and predicted labels\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(min(9, len(X_val))):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(imshow(X_val_np[i]))\n",
    "    \n",
    "    true_class_idx = y_val_np[i]\n",
    "    pred_class_idx = np.argmax(predictions_np[i])\n",
    "    \n",
    "    true_class = class_names[true_class_idx]\n",
    "    pred_class = class_names[pred_class_idx]\n",
    "    confidence = predictions_np[i][pred_class_idx] * 100\n",
    "    \n",
    "    title_color = 'green' if true_class_idx == pred_class_idx else 'red'\n",
    "    plt.title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.1f}%', \n",
    "              color=title_color)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91095e52",
   "metadata": {},
   "source": [
    "## 6. Model Saving\n",
    "\n",
    "Let's save the better performing model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the better model\n",
    "model_save_path = f\"{model_name.replace(' ', '_').lower()}_model.pt\"\n",
    "torch.save(better_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save class indices for future reference\n",
    "import json\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)\n",
    "print(\"Class indices saved to class_indices.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1cb61",
   "metadata": {},
   "source": [
    "## 7. Create a Function for Making Predictions on New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, class_names, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Predict the class of a new image using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        image_path: Path to the image file\n",
    "        class_names: Dictionary mapping class indices to class names\n",
    "        target_size: Size to resize the image to\n",
    "        \n",
    "    Returns:\n",
    "        Predicted class name and confidence\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Apply the same transformations used for validation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "    \n",
    "    # Convert to Python values\n",
    "    pred_class_idx = predicted_idx.item()\n",
    "    confidence_value = confidence.item() * 100\n",
    "    \n",
    "    # Get class name\n",
    "    pred_class_name = class_names[pred_class_idx]\n",
    "    \n",
    "    return pred_class_name, confidence_value\n",
    "\n",
    "# Test the prediction function on a sample image\n",
    "# Find a sample image\n",
    "sample_class = list(class_counts.keys())[0]  # Get the first class\n",
    "sample_class_path = os.path.join(dataset_path, sample_class)\n",
    "if os.path.isdir(sample_class_path):\n",
    "    image_files = [f for f in os.listdir(sample_class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if image_files:\n",
    "        sample_image_path = os.path.join(sample_class_path, image_files[0])\n",
    "        \n",
    "        # Predict\n",
    "        pred_class, confidence = predict_image(better_model, sample_image_path, class_names)\n",
    "        \n",
    "        # Display the image and prediction\n",
    "        img = cv2.imread(sample_image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Predicted: {pred_class} (Confidence: {confidence:.1f}%)\\nTrue: {sample_class}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Predicted class: {pred_class}\")\n",
    "        print(f\"Confidence: {confidence:.1f}%\")\n",
    "        print(f\"True class: {sample_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf539d8",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded and explored the 300_dataset of plant species images\n",
    "2. Preprocessed the images, including resizing and data augmentation\n",
    "3. Built two models:\n",
    "   - A simple CNN from scratch\n",
    "   - A transfer learning model using VGG16\n",
    "4. Trained and evaluated both models\n",
    "5. Visualized the results, including confusion matrices and example predictions\n",
    "6. Saved the better-performing model for future use\n",
    "7. Created a utility function for making predictions on new images\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- The transfer learning model generally performs better than the simple CNN model, which is expected since it leverages pre-trained weights on a large dataset (ImageNet).\n",
    "- Data augmentation helps to improve model generalization, especially when working with a limited dataset.\n",
    "- The confusion matrix reveals which classes are more difficult to distinguish, providing insights for potential model improvements.\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "1. Fine-tune the pre-trained layers of the VGG16 model\n",
    "2. Try other pre-trained architectures like ResNet, Inception, or EfficientNet\n",
    "3. Increase dataset size through more aggressive data augmentation\n",
    "4. Implement ensemble methods by combining predictions from multiple models\n",
    "5. Apply techniques to handle class imbalance if present in the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
