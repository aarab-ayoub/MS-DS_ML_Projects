{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30497d07",
   "metadata": {},
   "source": [
    "# Reconnaissance d'objets dans les images avec CNN (Réseaux de neurones convolutifs)\n",
    "## Dataset CIFAR-10\n",
    "Objectif c'est de <b>reconnaitre</b> des objets dans les images en utilisant <b>CNN</b> et le Dataset <b>CIFAR-10</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023c06c-c606-4ae0-bff6-d8d0fb86b66e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.9' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.13 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"cifar10.png\", width=400, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344820cc-4635-4f5d-9b0d-5eaf88c1d587",
   "metadata": {},
   "source": [
    "## 1. Installation et Configuration de TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03200ae4-a165-4f03-8920-c7e120d83ad3",
   "metadata": {},
   "source": [
    "### 1.1. Installaion de TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3ff2b-149d-4d69-96e5-5545e8c43665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e7ad7-c603-41ac-9f4d-8ee512bab08e",
   "metadata": {},
   "source": [
    "### 1.2. Installer TensorFlow pour CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a900fb-446a-4f04-9fa1-8a9442d830a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3acf94-6071-4893-82a0-012757335db2",
   "metadata": {},
   "source": [
    "### 1.3. Installer TensorFlow pour GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb4100-a91a-4391-951d-34c96fda96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow-gpu\n",
    "#_____________Ou____________\n",
    "!pip install -q tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642f618-a327-427d-a219-0aacc08eb305",
   "metadata": {},
   "source": [
    "### 1.4. Version TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9715d00-26dd-4154-bdd8-d3ccff448097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177d818-0ba2-45a0-8264-254c06eb0630",
   "metadata": {},
   "source": [
    "### 1.5. Autres packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np #Manipuler le Dataset comme matrice\n",
    "import matplotlib.pyplot as plt #Visualiser Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa071841-3b91-4752-8fcc-c210d66c0136",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "### 2.1. CIFAR-10 - Object Recognition in Images\n",
    "Dataset <b>CIFAR-10</b> : contient 60 000 images de <b>10 classes d'objets</b> (avions, voitures, oiseaux, chats, cerfs, chiens, grenouilles, chevaux, navires et camions). La répartition entre chaque classes est égale (6 000 images pour chaque classe). Les images sont en couleurs, mais en basse résolution (32x32 pixels)\n",
    "\n",
    "### 2.1.1. Import CIFAR-10 from Kaggel:\n",
    "https://www.kaggle.com/c/cifar-10/\n",
    "\n",
    "### 2.1.2. Import CIFAR-10 from tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451a4a3-4029-4c89-901a-2387c8d55a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4527379-2f41-45ed-b314-7cb8e0ab3adb",
   "metadata": {},
   "source": [
    "### 2.1.3. Charger le Dataset cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873fc83-009b-466e-a6d6-16874d07304d",
   "metadata": {},
   "source": [
    "### 2.1.4. Comment peut-on voir les images dans le dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b0388-ebd2-4293-b586-3581ee7c5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbr des images d'entrainement = 50 000\n",
    "#nbr des images de test = 10 000\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb4743-47a5-4352-9c21-d01605bbfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimenssion de x_train\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543ba6f-2524-4b90-8519-086bb0e8e1d0",
   "metadata": {},
   "source": [
    "<b>50000</b> images d'<b>entraînement</b> et <b>10000</b> images de test\n",
    "Les images sont de taille <b>32x32</b> et ont <b>3</b> canaux de couleur <b>(Red, Green, Blue)</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107dabf-5bf7-4741-836f-ac425be92f15",
   "metadata": {},
   "source": [
    "Chaque <b>ligne</b> du tableau <b>x_train</b> stocke une image couleur 32x32. Les <b>1024 premières</b> entrées contiennent les valeurs du canal <b>rouge</b>, les <b>1024 suivantes</b> les valeurs <b>vertes</b> et les <b>1024 dernières</b> les valeurs <b>bleues</b>. L'image est stockée dans l'<b>ordre</b> des lignes principales, de sorte que les <b>32 premières entrées</b> du tableau sont les valeurs du canal <b>rouge</b> de la première <b>ligne</b> de l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567037b-2453-4b2a-bd5e-c0d1d44be6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"imgMatrix.png\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1773c8-5a87-47da-9857-eb50949b9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#couleurs entre 0 et 255\n",
    "x_train.max(), x_train.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3ac59-fc5a-4a3b-ac8f-9cbab8ef4af9",
   "metadata": {},
   "source": [
    "La plages des couleurs est comprise entre <b>0</b> et <b>255</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad1481-1ed8-4c0a-937f-869a2dfc4b3e",
   "metadata": {},
   "source": [
    "### 2.1.5. Noms des classes du Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d4b26-69c6-448c-b058-aacc440d2874",
   "metadata": {},
   "source": [
    "Il y a <b>10 classes</b> des d'objets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbecb2f-35b5-4c68-9915-989aac6e8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.max(), y_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805afbf3-6a5d-493e-862a-be3c7a5e5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Les noms des classes\n",
    "class_names = ['0: airplane', '1: automobile', '2: bird', '3: cat', '4: deer', \n",
    "               '5: dog', '6: frog', '7: horse', '8: ship', '9: truck']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2ca7d-e145-41f2-b5b9-8f515943be5c",
   "metadata": {},
   "source": [
    "### 2.1.6. Normalisation des données\n",
    "Les <b>pixels</b> de l'image sont dans une plage de <b>0 à 255</b>, et nous devons <b>réduire</b> ces valeurs à une valeur comprise entre <b>0 et 1</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007da8d6-87d5-4af8-820b-c607074827f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des images\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278d840-fd6c-4163-be68-6d76bdb85123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valeur comprise entre 0 et 1\n",
    "x_train.max(), x_train.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f132c9d-abbc-43e9-b840-438b352624e8",
   "metadata": {},
   "source": [
    "### 2.1.7. Visualisation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca6c8d-5d3a-4f46-9153-bc2e71053bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# image de la ligne 1\n",
    "plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e298f-f1a2-4f06-8c00-d587e6ff05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classe de l'image de la ligne 1\n",
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576ec3b-451b-4109-a67f-7f3f6316e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bccd66-9d6c-4722-87a4-2c571e8f8759",
   "metadata": {},
   "source": [
    "#### classe 9 -> truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b8246-eb65-44d5-bf86-5419be8b9fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiser 25 premières images\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "img = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i][j].imshow(x_train[img])\n",
    "        img += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c3855-9671-4485-a704-b95c6c0f6e72",
   "metadata": {},
   "source": [
    "## 3. CNN (Réseaux de Noreunes Convolutif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e50b5-e155-4a5f-90de-d10ddfa87dd8",
   "metadata": {},
   "source": [
    "### 3.1. Définition de l'objet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476f655-19b6-4fc1-8a4d-34e664270b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4efe7-8b07-41d8-8b26-8725f2526a56",
   "metadata": {},
   "source": [
    "### 3.2. Construire le modèle\n",
    "1- Nous allons utiliser un <b>CNN</b> pour entraîner le <b>modèle</b>,<br>\n",
    "2- Cela comprend l'utilisation d'une couche de <b>convolution</b>, qui est la couche <b>Conv2d</b>,<br> \n",
    "3- Ainsi que des méthodes de <b>regroupement (pooling)</b> et de <b>normalisation</b>,<br>\n",
    "4- Enfin, nous le passerons dans une <b>couche dense</b> et la couche <b>dense finale</b> qui est notre couche de <b>sortie</b>, <br>\n",
    "5- Nous utilisons la fonction d'activation <b>relu</b>,<br> \n",
    "6- La couche de sortie utilise la fonction d'activation <b>softmax</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac137e4-5b26-4971-9759-08ed782fa28a",
   "metadata": {},
   "source": [
    "### 3.3. Ajout de la première couche Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a7236-8038-42b2-954d-ed184813c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) filters (kernel) = 32\n",
    "# 2) kernal size = 3\n",
    "# 3) padding = same\n",
    "# 4) activation = ReLU\n",
    "# 5) input shape = (32, 32, 3)\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape = [32, 32, 3]))\n",
    "#model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d1256-1ca1-4dfc-8a6d-dbded0e50259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Architecture AlexNet\n",
    "Image(filename = \"alexnet.png\", width=1000, height=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3a3ca-0b9b-41d1-a8ca-d66aef2eb207",
   "metadata": {},
   "source": [
    "### 3.4. Padding\n",
    "<b>Padding : </b> est une technique utilisée par <b>CNN</b> pour préserver les dimensions spatiales des données d'entrée et éviter la perte d'informations sur les bords de l'image.<br>\n",
    "Il consiste à <b>ajouter</b> des <b>lignes</b> et/ou des <b>colonnes</b> de pixels supplémentaires autour des bords des données d'entrée <b>avant convolution</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b437e-a570-4308-88cf-f8a5c8e456fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"padding0.png\", width=200, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88ca5a-78d1-42ca-a014-22e21ec450e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"SVpadding.gif\", width=300, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df000d-39fa-4c59-8e64-09723b3af611",
   "metadata": {},
   "source": [
    "#### 3.4.1. Same Padding\n",
    "#### On ne perd pas des infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307146cd-e2c6-4858-8df4-579136025f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(filename = \"padding1.png\", width=400, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea5ae4-f08f-4e87-966c-3658f0d63aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"padding2.png\", width=400, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a881ce-9f90-4480-9b04-8cfb264678d3",
   "metadata": {},
   "source": [
    "#### 3.4.2. Valid Padding\n",
    "#### On perd des infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed9a25-f5ec-4212-ba95-33712ba59ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(filename = \"padding3.png\", width=400, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e53ef6-e893-4bf6-9055-8cfd2a2e698a",
   "metadata": {},
   "source": [
    "### 3.5. Ajout de la deuxième couche Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a89a53-5025-40d0-9536-5d6b2904850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) filters (kernel) = 32\n",
    "# 2) kernal size = 3\n",
    "# 3) padding = same\n",
    "# 4) activation = ReLU\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf3b09-6095-4483-9333-71ceeae29369",
   "metadata": {},
   "source": [
    "### 3.6. Ajout de la 3ème couche maxpool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15a592-5d7d-4ea3-b069-b49958888c46",
   "metadata": {},
   "source": [
    "<b>Pooling</b> : utilisées pour réduire les dimensions des <b>features</b>.</br>\n",
    "La technique la plus utilisée est <b>Max Pool</b> : permettant de sélectionner l'élément <b>maximal</b> de la région du <b>feature</b> couverte par le <b>filtre</b>.<br>\n",
    "Il existe une autre technique <b>Average Pool</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb787103-12ee-48ed-903c-0157dc13045f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Max Pool\n",
    "Image(filename = \"pooling.png\", width=400, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59b3ca-8ef4-4aed-9d71-5e4135695fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Pool\n",
    "Image(filename = \"pooling2.png\", width=400, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed386bb8-c4d4-46a8-a8e1-91bf90f70c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxpool layer parameters,\n",
    "# 1) pool size = 2\n",
    "# 2) strides = 2\n",
    "# 3) padding = valid\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e224146-77d0-4b71-8e71-4fce8a2e814a",
   "metadata": {},
   "source": [
    "### 3.7. Ajout de la 4ème couche Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecbb301-c073-489c-915e-be89cb747ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) filters (kernel) = 64\n",
    "# 2) kernal size = 3\n",
    "# 3) padding = same\n",
    "# 4) activation = ReLU\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684d68f-28ab-4a3b-a0f8-17501cf6bd29",
   "metadata": {},
   "source": [
    "### 3.8. Ajout de la 5ème couche Conv2D et la 6ème couche maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6128e-a28d-4fd6-be67-7b81e2b0e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) filters (kernel) = 64\n",
    "# 2) kernal size = 3\n",
    "# 3) padding = same\n",
    "# 4) activation = ReLU\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "# maxpool layer parameters,\n",
    "# 1) pool size = 2\n",
    "# 2) strides = 2\n",
    "# 3) padding = valid\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7dd976-73e0-4b8f-864d-b25509f40dbf",
   "metadata": {},
   "source": [
    "### 3.9 Dropout:\n",
    "L’<b>overfitting</b> est un problème fréquent lors de l’entraînement d’un <b>modèle de Deep Learning.</b><br>\n",
    "Pour résoudre ce type de problème on utilise la technique de <b>Dropout.</b><br>\n",
    "C’est une méthode de <b>Regularization</b> : qui permet à la fois d’<b>optimiser l’apprentissage</b> d’un modèle de Deep Learning et de <b>contrer l’overfitting.</b><br>\n",
    "<b>Dropout :</b>c'est la <b>suppression de neurones</b> dans les couches d’un modèle de Deep Learning.<br>\n",
    "On <b>désactive temporairement</b> certains neurones dans le réseau, ainsi que <b>toutes ses connexions</b> entrantes et sortantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04457dc6-a46c-48ba-922b-98e7c4b0a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"dropoutnet.png\", width=500, height=250) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146d769-d9e9-4026-bc1e-6c9cda85ef7b",
   "metadata": {},
   "source": [
    "Le choix des <b>neurones à désactiver est aléatoire</b>. On attribue une <b>probabilité p</b> à tous les neurones qui détermine leur activation.<br>\n",
    "Lorsque <b>p = 0.1</b>, chaque neurone a une chance sur <b>10 d’être désactivé</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf13e20-b67b-4a14-a947-a20f18cf3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the dropout layer\n",
    "model.add(tf.keras.layers.Dropout(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd774d-2187-49cf-82b6-4015f8903477",
   "metadata": {},
   "source": [
    "### 3.10. Flattening layer\n",
    "La couche <b>Flattening layer</b> est utilisée pour convertir une sortie <b>multidimensionnelle</b> de la couche précédente en un <b>vecteur unidimensionnel</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99f7cb-8947-477c-bab8-c20827068aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the Flattening layer\n",
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b12447-1ccf-470e-a911-eba7349a8307",
   "metadata": {},
   "source": [
    "### 3.11. Dense layer\n",
    "La <b>couche dense</b> reçoit des entrées de tous les neurones précédents. Il forme donc une couche dense. Cette couche représente la <b>multiplication de vecteurs de matrice</b>.<br>\n",
    "Les couches denses bien adaptées à des tâches telles que la <b>classification d'images</b>, où l'entrée est une image de <b>grande dimension</b> et la sortie est une <b>prédiction</b> de la <b>classe</b> de l'objet dans l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0b6b9-247e-4824-acf2-5209c321ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding first dense layer\n",
    "model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Adding second dense layer (output layer)\n",
    "#units=10 --> classes\n",
    "model.add(tf.keras.layers.Dense(units=10, activation='softmax')) #couche de sortie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf1182-3e25-4981-980e-dc6b6513f9ca",
   "metadata": {},
   "source": [
    "### 3.12 Résumé du CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f39558-4a87-4459-b1f0-fdf962a59de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c81967-e41b-4df3-bed2-dddf3151689d",
   "metadata": {},
   "source": [
    "## 4. Entrainement de CNN\n",
    "### 4.1. Compiler le CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e947a7-13ab-49bd-b7ac-9e0f30aa4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = loss function\n",
    "#optimizer = optimiser loss function\n",
    "#metrics = accuracy du CNN\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28835299-a3f3-4820-b73c-18c7f958a166",
   "metadata": {},
   "source": [
    "### 4.2. Entrainer le CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8b031-a5a0-476f-bd98-e92f4a4f6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83878c-5537-4181-a318-be0c624bf685",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Remarque : <font>\n",
    "Pour obtenir une <font color=\"red\"><b>meilleur accuracy</b></font> on peut changer les paramètres comme : <b>epochs</b>, <b>nbr de cnn layers</b>, <b>nbr de maxpool layers</b> et la <b>probabilité de droupout layer</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8316de-698c-4246-b0ef-6e692bf0f2b2",
   "metadata": {},
   "source": [
    "## 5. Evaluation la pérformance du CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36220c-25b1-4d54-a743-d3c8ef0546cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b426a6-744a-44e1-b5d8-bc08c58e23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy est : ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef000c14-be3c-49fd-a7a1-c6f4c0c59f73",
   "metadata": {},
   "source": [
    "## 6. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d858a6e-3bdd-4ac0-9b55-570d9613d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict_classes(x_test)\n",
    "#y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "#y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(model.predict(x_test),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed2f12-3f5a-40a9-9b51-7b31e995763c",
   "metadata": {},
   "source": [
    "### 6.1. Prediction d'une image individuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da81c26-7ab9-47e1-bb0f-60d4977143da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[2]), print(y_test[2]) #image 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a138b5cd-c63b-44f6-93d8-2f6b7e4ca2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[50]), print(y_test[50]) #image 50\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd9cc7-62d8-4213-8e37-4ca50842d1bf",
   "metadata": {},
   "source": [
    "### 6.2. Matrice de confusion \n",
    "La <b>matrice de confusion</b> (ou matrice d'<b>erreur</b>) est une méthode qui permet de visualiser les <b>résultats</b> d'un algorithme de <b>classification</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52744ac-e190-4b9f-9976-e3223f1e8fee",
   "metadata": {},
   "source": [
    "#### 6.2.1. Comment interpréter une matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f6d8c-e6f4-442a-86f0-d5146ec7c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification binaire\n",
    "Image(filename = \"cmb.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e081e-78b9-4dcd-a1fd-8253699f79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification multiple\n",
    "Image(filename = \"cmm.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcbb7a-b4ef-4d29-9fdb-ccb462b58e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage de la matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a30139-9021-4410-9806-7a8ab2ada72b",
   "metadata": {},
   "source": [
    "#### 6.2.2. Calcule de l'accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f695a0-81e0-4be3-a84e-77dee93839b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_cm = accuracy_score(y_test, y_pred)\n",
    "print(acc_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2444fce-fc28-42df-923e-5bbc063502f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcule de l'accuracy avec la matrice de confusion\n",
    "Image(filename = \"cmba.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f682c01-28e6-45c4-b817-de7c04505e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(cm[i,i] for i in range(10))/np.sum(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61330d52-ab0f-40e9-bfee-43cde78b4104",
   "metadata": {},
   "source": [
    "### 7. <font color = \"red\">Learning Curve ---> Après</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27c9d6-0b7a-49fd-adb3-63a9c97a311b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
