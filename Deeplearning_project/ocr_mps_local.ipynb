{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcccae0",
   "metadata": {},
   "source": [
    "# üçé OCR Training - Apple MPS Optimized\n",
    "\n",
    "**Optimized for Mac with Apple Silicon (M1/M2/M3)**\n",
    "\n",
    "## Key Settings:\n",
    "- ‚úÖ MPS device acceleration\n",
    "- ‚úÖ IMG_WIDTH = 1024 (stable)\n",
    "- ‚úÖ Batch size = 16\n",
    "- ‚úÖ Light augmentation\n",
    "- ‚úÖ Conservative learning rate\n",
    "\n",
    "**Dataset:** `./2/dataset/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a602831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using Apple MPS (Metal Performance Shaders)\n",
      "Device: mps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MPS setup for Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('‚úÖ Using Apple MPS (Metal Performance Shaders)')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('‚úÖ Using CUDA')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('‚ö†Ô∏è  Using CPU')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "        \n",
    "set_seed(42)\n",
    "print(f'Device: {device}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee233fe8",
   "metadata": {},
   "source": [
    "## Load Local Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c903a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: ./2/dataset/\n",
      "Images dir: ./2/dataset/images\n",
      "Labels: ./2/dataset/labels.csv\n",
      "\n",
      "Total rows in CSV: 20000\n",
      "Valid samples: 6137\n",
      "Max text length: 130\n",
      "\n",
      "Sample entry:\n",
      "LIST_0 ‚Üê [ 55 , 53 , 55 , 50 ]\n",
      "VAR_0 ‚Üê taille ( LIST_0 )\n"
     ]
    }
   ],
   "source": [
    "# Local paths\n",
    "BASE_PATH = './2/dataset/'\n",
    "IMG_DIR = os.path.join(BASE_PATH, 'images')\n",
    "labels_path = os.path.join(BASE_PATH, 'labels.csv')\n",
    "\n",
    "print(f'Loading from: {BASE_PATH}')\n",
    "print(f'Images dir: {IMG_DIR}')\n",
    "print(f'Labels: {labels_path}\\n')\n",
    "\n",
    "df = pd.read_csv(labels_path)\n",
    "print(f'Total rows in CSV: {len(df)}')\n",
    "\n",
    "# Clean data\n",
    "df = df[df['text'].notna()].reset_index(drop=True)\n",
    "df = df[df['text'].str.strip() != ''].reset_index(drop=True)\n",
    "\n",
    "# Filter by length\n",
    "MAX_TEXT_LEN = 130\n",
    "df = df[df['text'].str.len() <= MAX_TEXT_LEN].reset_index(drop=True)\n",
    "\n",
    "# Verify images exist\n",
    "df['img_path'] = df['file_name'].apply(lambda x: os.path.join(IMG_DIR, x))\n",
    "df['exists'] = df['img_path'].apply(os.path.isfile)\n",
    "df = df[df['exists']].reset_index(drop=True)\n",
    "\n",
    "print(f'Valid samples: {len(df)}')\n",
    "print(f'Max text length: {df[\"text\"].str.len().max()}')\n",
    "print(f'\\nSample entry:')\n",
    "print(df.iloc[0]['text'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafe522",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7489c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 57\n",
      "Characters: \t\n",
      " ()*+,-/0123456789<=>ADILRSTVX[]_adefghilmnopqrs...\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique characters\n",
    "unique_chars = sorted(set(''.join(df['text'].tolist())))\n",
    "char_list = ['<blank>'] + unique_chars  # blank for CTC\n",
    "char_to_idx = {ch: i for i, ch in enumerate(char_list)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(char_list)}\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "print(f'Characters: {\"\".join(unique_chars[:50])}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708dab92",
   "metadata": {},
   "source": [
    "## Configuration - OPTIMIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0405c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 64x1024\n",
      "Expected sequence length: ~256\n",
      "Batch size: 16\n",
      "Epochs: 40 (patience: 12)\n"
     ]
    }
   ],
   "source": [
    "# Image config - FIXED for stability\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 1024  # ‚úÖ Reduced from 3200\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 16   # ‚úÖ Increased from 8\n",
    "NUM_EPOCHS = 40\n",
    "PATIENCE = 12\n",
    "\n",
    "print(f'Image size: {IMG_HEIGHT}x{IMG_WIDTH}')\n",
    "print(f'Expected sequence length: ~{IMG_WIDTH // 4}')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Epochs: {NUM_EPOCHS} (patience: {PATIENCE})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7ded9",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61cb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4909, Val: 1228\n",
      "Train batches: 307, Val batches: 77\n"
     ]
    }
   ],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, dataframe, char_to_idx, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['img_path']).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        encoded = [self.char_to_idx[ch] for ch in row['text']]\n",
    "        return img, torch.tensor(encoded, dtype=torch.long), len(encoded)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, texts, lens = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    lens = torch.tensor(lens, dtype=torch.long)\n",
    "    max_len = max(lens)\n",
    "    padded = torch.zeros(len(texts), max_len, dtype=torch.long)\n",
    "    for i, text in enumerate(texts):\n",
    "        padded[i, :len(text)] = text\n",
    "    return imgs, padded, lens\n",
    "\n",
    "# Light augmentation - text friendly\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.1, hue=0.0)\n",
    "    ], p=0.5),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3, sigma=(0.1, 0.5))], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split 80/20\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "\n",
    "train_idx, val_idx = random_split(\n",
    "    range(len(df)), [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_ds = OCRDataset(df.iloc[train_idx.indices], char_to_idx, train_transform)\n",
    "val_ds = OCRDataset(df.iloc[val_idx.indices], char_to_idx, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}')\n",
    "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f623b53",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edc12b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded\n",
      "Parameters: 5,449,017\n",
      "Output shape: torch.Size([256, 2, 57])  (T=256, B=2, C=57)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class ImprovedOCR(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim=256, nhead=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 32x512\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 16x256\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None))  # 1xW\n",
    "        )\n",
    "        \n",
    "        self.proj = nn.Linear(512, hidden_dim)\n",
    "        self.pos_enc = PositionalEncoding(hidden_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=nhead, \n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.2, activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # [B, 512, 1, W]\n",
    "        x = x.squeeze(2).permute(0, 2, 1)  # [B, W, 512]\n",
    "        x = self.proj(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x.permute(1, 0, 2)  # [T, B, C] for CTC\n",
    "\n",
    "model = ImprovedOCR(num_classes=vocab_size, hidden_dim=256, nhead=8, num_layers=4).to(device)\n",
    "\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f'‚úÖ Model loaded')\n",
    "print(f'Parameters: {params:,}')\n",
    "\n",
    "# Test forward pass\n",
    "test_in = torch.randn(2, 3, IMG_HEIGHT, IMG_WIDTH).to(device)\n",
    "test_out = model(test_in)\n",
    "print(f'Output shape: {test_out.shape}  (T={test_out.shape[0]}, B={test_out.shape[1]}, C={test_out.shape[2]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b000d80",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e1e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training setup complete\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-4,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3\n",
    ")\n",
    "\n",
    "def decode(logits, idx_to_char):\n",
    "    preds = []\n",
    "    logits = logits.permute(1, 0, 2)\n",
    "    for i in range(logits.size(0)):\n",
    "        indices = torch.argmax(logits[i], dim=-1).tolist()\n",
    "        decoded, prev = [], None\n",
    "        for idx in indices:\n",
    "            if idx != 0 and idx != prev:\n",
    "                decoded.append(idx_to_char.get(idx, ''))\n",
    "            prev = idx\n",
    "        preds.append(''.join(decoded))\n",
    "    return preds\n",
    "\n",
    "def accuracy(preds, targets, lens):\n",
    "    total, correct = 0, 0\n",
    "    for pred, target, l in zip(preds, targets, lens):\n",
    "        true = ''.join([idx_to_char[target[j].item()] for j in range(l)])\n",
    "        total += len(true)\n",
    "        for i, c in enumerate(true):\n",
    "            if i < len(pred) and pred[i] == c:\n",
    "                correct += 1\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "print('‚úÖ Training setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b2253",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training on MPS...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_ctc_loss' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 449b1768410104d3ed79d3bcfe4ba1d65c7f22c0. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m in_lens = torch.full((imgs.size(\u001b[32m0\u001b[39m),), seq_len, dtype=torch.long)\n\u001b[32m     31\u001b[39m log_probs = F.log_softmax(logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(loss) \u001b[38;5;129;01mor\u001b[39;00m loss.item() > \u001b[32m100\u001b[39m:\n\u001b[32m     35\u001b[39m     skipped += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/nn/modules/loss.py:2070\u001b[39m, in \u001b[36mCTCLoss.forward\u001b[39m\u001b[34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[39m\n\u001b[32m   2062\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   2063\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2064\u001b[39m     log_probs: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2067\u001b[39m     target_lengths: Tensor,\n\u001b[32m   2068\u001b[39m ) -> Tensor:\n\u001b[32m   2069\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/nn/functional.py:3107\u001b[39m, in \u001b[36mctc_loss\u001b[39m\u001b[34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[39m\n\u001b[32m   3095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[32m   3096\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   3097\u001b[39m         ctc_loss,\n\u001b[32m   3098\u001b[39m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[32m   (...)\u001b[39m\u001b[32m   3105\u001b[39m         zero_infinity=zero_infinity,\n\u001b[32m   3106\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3107\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::_ctc_loss' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 449b1768410104d3ed79d3bcfe4ba1d65c7f22c0. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "print('üöÄ Training on MPS...\\n')\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    t_loss = 0\n",
    "    t_preds, t_targets, t_lens = [], [], []\n",
    "    valid = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{NUM_EPOCHS}', leave=False)\n",
    "    for imgs, targets, lens in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_out = model(imgs[:1])\n",
    "            seq_len = test_out.size(0)\n",
    "        \n",
    "        if (lens > seq_len).any():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        in_lens = torch.full((imgs.size(0),), seq_len, dtype=torch.long)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        # Move to CPU for CTC loss (MPS doesn't support CTC yet)\n",
    "        loss = criterion(log_probs.cpu(), targets, in_lens, lens)\n",
    "        \n",
    "        if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 100:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        t_loss += loss.item()\n",
    "        valid += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            t_preds.extend(decode(logits.cpu(), idx_to_char))\n",
    "            t_targets.extend(targets)\n",
    "            t_lens.extend(lens)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
    "    \n",
    "    if valid == 0:\n",
    "        print(f'Epoch {epoch}: No valid batches!')\n",
    "        continue\n",
    "    \n",
    "    avg_t = t_loss / valid\n",
    "    t_acc = accuracy(t_preds, t_targets, t_lens)\n",
    "    train_losses.append(avg_t)\n",
    "    train_accs.append(t_acc)\n",
    "    \n",
    "    # VAL\n",
    "    model.eval()\n",
    "    v_loss = 0\n",
    "    v_preds, v_targets, v_lens = [], [], []\n",
    "    valid_v = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, targets, lens in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            seq_len = logits.size(0)\n",
    "            in_lens = torch.full((imgs.size(0),), seq_len, dtype=torch.long)\n",
    "            \n",
    "            if (lens > seq_len).any():\n",
    "                continue\n",
    "            \n",
    "            # Move to CPU for CTC loss (MPS doesn't support CTC yet)\n",
    "            loss = criterion(log_probs.cpu(), targets, in_lens, lens)\n",
    "            \n",
    "            if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() < 100:\n",
    "                v_loss += loss.item()\n",
    "                valid_v += 1\n",
    "            \n",
    "            v_preds.extend(decode(logits.cpu(), idx_to_char))\n",
    "            v_targets.extend(targets)\n",
    "            v_lens.extend(lens)\n",
    "    \n",
    "    avg_v = v_loss / valid_v if valid_v > 0 else float('inf')\n",
    "    v_acc = accuracy(v_preds, v_targets, v_lens)\n",
    "    val_losses.append(avg_v)\n",
    "    val_accs.append(v_acc)\n",
    "    \n",
    "    if v_acc > best_val_acc:\n",
    "        best_val_acc = v_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_mps_model.pth')\n",
    "        print(f'‚úÖ Best: {v_acc*100:.2f}% (epoch {epoch})')\n",
    "    \n",
    "    skip_msg = f' | Skip: {skipped}' if skipped > 0 else ''\n",
    "    print(f'Epoch {epoch:2d}/{NUM_EPOCHS} | Train: {avg_t:.4f}/{t_acc*100:.2f}% | Val: {avg_v:.4f}/{v_acc*100:.2f}%{skip_msg}')\n",
    "    \n",
    "    if epoch - best_epoch >= PATIENCE:\n",
    "        print(f'\\nüõë Early stop (no improvement for {PATIENCE} epochs)')\n",
    "        print(f'Best: {best_val_acc*100:.2f}% at epoch {best_epoch}')\n",
    "        break\n",
    "\n",
    "print(f'\\n‚úÖ Training complete!')\n",
    "\n",
    "print(f'Best validation accuracy: {best_val_acc*100:.2f}% (epoch {best_epoch})')print(f'Best validation accuracy: {best_val_acc*100:.2f}% (epoch {best_epoch})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b25ae4",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fa3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, 'o-', label='Train', lw=2)\n",
    "plt.plot(val_losses, 's-', label='Val', lw=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([a*100 for a in train_accs], 'o-', label='Train', lw=2)\n",
    "plt.plot([a*100 for a in val_accs], 's-', label='Val', lw=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Character Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mps_training_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'Best: {best_val_acc*100:.2f}% at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138abe9",
   "metadata": {},
   "source": [
    "## Evaluate & Show Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_mps_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "final_preds, final_targets, final_lens = [], [], []\n",
    "\n",
    "print('Evaluating on validation set...\\n')\n",
    "with torch.no_grad():\n",
    "    for imgs, targets, lens in tqdm(val_loader, desc='Eval'):\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        final_preds.extend(decode(logits.cpu(), idx_to_char))\n",
    "        final_targets.extend(targets)\n",
    "        final_lens.extend(lens)\n",
    "\n",
    "final_acc = accuracy(final_preds, final_targets, final_lens)\n",
    "\n",
    "print(f'\\nüéØ FINAL ACCURACY: {final_acc*100:.2f}%\\n')\n",
    "print('='*100)\n",
    "print('Sample Predictions:\\n')\n",
    "\n",
    "for i in range(min(15, len(final_preds))):\n",
    "    true = ''.join([idx_to_char[final_targets[i][j].item()] for j in range(final_lens[i])])\n",
    "    pred = final_preds[i]\n",
    "    char_acc = sum(1 for a,b in zip(true, pred) if a==b) / len(true) * 100 if len(true) > 0 else 0\n",
    "    \n",
    "    print(f'Sample {i+1:2d} | Acc: {char_acc:5.1f}%')\n",
    "    print(f'  True: {true[:90]}')\n",
    "    print(f'  Pred: {pred[:90]}')\n",
    "    print('-'*100)\n",
    "\n",
    "print(f'\\n‚úÖ Final Accuracy: {final_acc*100:.2f}%')\n",
    "print(f'Model saved as: best_mps_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
